{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "## Mean-Shift Clustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "## feature\n",
    "import datetime\n",
    "\n",
    "## train-test split\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Select 2k points\n",
    "def select_data_trajecory(df,k):\n",
    "    ## Sort POLYLINE\n",
    "    df.POLYLINE = df.POLYLINE.apply(json.loads)\n",
    "    ## Find records > 10 times\n",
    "    return df[df.POLYLINE.apply(len)>(2*k)]\n",
    "\n",
    "## Extract Destination\n",
    "def extract_dest_row(s):\n",
    "    ## return new rows\n",
    "    return pd.Series({'trip_id': s[0], 'longitude': s[4][-1][1], 'latitude': s[4][-1][0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Mean-Shift Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trajecory_meanshift(df_destination):    \n",
    "    ## Generate data\n",
    "    X = df_destination.iloc[:,0:2].values\n",
    "    \n",
    "    # Compute clustering with MeanShift\n",
    "\n",
    "    # The following bandwidth can be automatically detected using\n",
    "    bandwidth = estimate_bandwidth(X, quantile=0.05, n_samples=1000)\n",
    "\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(X)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "    \n",
    "    ## Add Label\n",
    "    df_destination = pd.concat([df_destination.reset_index().drop(['index'],axis=1), \n",
    "                                pd.Series(ms.labels_)], axis=1)\n",
    "    ## Sort Order\n",
    "    df_destination.columns = ['latitude','longitude','trip_id','label']\n",
    "    df_destination = df_destination[['trip_id','latitude','longitude','label']]\n",
    "\n",
    "    return df_destination\n",
    "    # ###############################################################################\n",
    "    # # Plot result\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # from itertools import cycle\n",
    "\n",
    "    # plt.figure(1)\n",
    "    # plt.clf()\n",
    "\n",
    "    # colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "    # for k, col in zip(range(n_clusters_), colors):\n",
    "    #     my_members = labels == k\n",
    "    #     cluster_center = cluster_centers[k]\n",
    "    #     plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "    #     plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "    #              markeredgecolor='k', markersize=14)\n",
    "    # plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Categorical Feature Function\n",
    "def feature_sort_datetime(df):    \n",
    "    #### FDT_DATE change to date format\n",
    "    df.FDT_DATE = df.FDT_DATE.apply(lambda x:datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "    ## extract datetime value\n",
    "    df['monthday'] = df.FDT_DATE.apply(lambda x:x.day)\n",
    "    df['weekday'] = df.FDT_DATE.apply(lambda x:x.isocalendar()[2])\n",
    "    df['quater'] = df.FDT_DATE.apply(lambda x:(x.hour*4 + x.minute/15))\n",
    "    df['hour'] = df.FDT_DATE.apply(lambda x:x.hour)\n",
    "    df['minute'] = df.FDT_DATE.apply(lambda x:x.minute)\n",
    "    df['second'] = df.FDT_DATE.apply(lambda x:x.second)\n",
    "    ## add some feature tricks\n",
    "    return df\n",
    "    \n",
    "## One-Hot Encoder\n",
    "def feature_ohencoder(df, n_category):\n",
    "    ## Converts categorical variables which less than n_categories into dummy variables.\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    output = pd.DataFrame(index = df.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in df.iteritems():\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        if len(np.unique(col_data)) < n_category:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)  \n",
    "        \n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "## Embedding\n",
    "def feature_embedding(df, n_category):\n",
    "    ## Converts categorical variables which more than n_categories with embedding matrix.\n",
    "    \n",
    "    # Initialize new output DataFrame\n",
    "    df = df.reset_index().drop('index',axis=1)\n",
    "    output = pd.DataFrame(index = df.index)\n",
    "\n",
    "    # Investigate each feature column for the data\n",
    "    for col, col_data in df.iteritems():\n",
    "\n",
    "        # If data type is categorical, convert to dummy variables\n",
    "        n_dim = len(np.unique(col_data))\n",
    "        if n_dim > n_category:\n",
    "            # Example: 'school' => 'school_GP' and 'school_MS'\n",
    "            col_data = pd.get_dummies(col_data, prefix = col)\n",
    "            # Embedding Matrix\n",
    "            np.random.seed(42)\n",
    "            matrix_emd = np.random.random((n_dim, n_category))\n",
    "            col_data = pd.DataFrame(np.dot(col_data, matrix_emd))\n",
    "        # Collect the revised columns\n",
    "        output = output.join(col_data)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Continues Feature Function\n",
    "## Selecting 10 POLYLINE points as trajecory （５ first, 5 last） and drop destination points\n",
    "def extract_points(s):\n",
    "    ## s is one polyline\n",
    "    ## return 10 points, except destination\n",
    "    return pd.Series({'lat0': s[0][0], 'lat1': s[1][0], 'lat2': s[2][0], 'lat3': s[3][0], 'lat4': s[4][0], \n",
    "                      'lat5': s[-6][0], 'lat6': s[-5][0], 'lat7': s[-4][0], 'lat8': s[-3][0], 'lat9': s[-2][0], \n",
    "                      'lon0': s[0][1], 'lon1': s[1][1], 'lon2': s[2][1], 'lon3': s[3][1], 'lon4': s[4][1], \n",
    "                      'lon5': s[-6][1], 'lon6': s[-5][1], 'lon7': s[-4][1], 'lon8': s[-3][1], 'lon9': s[-2][1]\n",
    "                     })\n",
    "\n",
    "def feature_points(df):\n",
    "    ## Extract POLYLINE points\n",
    "    return df.apply(extract_points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######## Normalization and Other Process\n",
    "## drop index and label\n",
    "def feature_cleaning(df):\n",
    "    return df.drop(['GROUP_ID','FSTR_ID','FDT_DATE','POLYLINE'], axis=1)\n",
    "\n",
    "## calculate normalization\n",
    "def normalize(feature):\n",
    "    return (feature-np.min(feature))/(np.max(feature)-np.min(feature))\n",
    "\n",
    "## apply normalization to numerical variables\n",
    "def feature_normalization(df):\n",
    "    return df.apply(normalize,axis=0)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of estimated clusters : 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/pandas/core/generic.py:2701: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/jiandong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## Load all trajecory data\n",
    "df_all_trajecory = pd.read_csv('df_all_trajecory.csv')\n",
    "\n",
    "## Select dataframe which has at least 2k points in POLYLINE\n",
    "df_trajecory = select_data_trajecory(df_all_trajecory, k=5)\n",
    "\n",
    "## Extract label and save\n",
    "df_destination = df_trajecory.apply(lambda s:extract_dest_row(s), axis=1)\n",
    "#df_destination.to_csv('all_label.csv',index=False)\n",
    "\n",
    "## Use Meanshift to cluster destinations, use trajecory_meanshift to add cluster columns\n",
    "df_destination = trajecory_meanshift(df_destination)\n",
    "\n",
    "######## Categorical Feature\n",
    "## Sort feature: datetime\n",
    "df_trajecory = feature_sort_datetime(df_trajecory)\n",
    "df_trajecory_categorical = df_trajecory.drop(['GROUP_ID','FSTR_ID','FDT_DATE','POLYLINE',\n",
    "                                           'hour','minute','second'],axis=1)\n",
    "## Sort feature: Categorical, less than 10 catercory turn to one-hot encoder\n",
    "## and espescially for meta data, use embedding convert to 10 dims\n",
    "df_trajecory_categorical = feature_ohencoder(df_trajecory_categorical, 10)\n",
    "## Sort feature: Embedding, only for meta data\n",
    "df_trajecory_categorical = feature_embedding(df_trajecory_categorical, 10)\n",
    "\n",
    "######## Numerical Feature\n",
    "## Sort feature: Extract points\n",
    "df_trajecory_numerical = feature_points(df_trajecory.POLYLINE)\n",
    "## Sort feature: Other numerical data\n",
    "df_trajecory_numerical = pd.concat([df_trajecory_numerical, \n",
    "                                    df_trajecory.loc[:,['hour','minute','second']]],axis=1)\n",
    "## Sort feature: Normalization\n",
    "df_trajecory_numerical = feature_normalization(df_trajecory_numerical)\n",
    "df_trajecory_numerical = df_trajecory_numerical.reset_index().drop('index',axis=1)\n",
    "\n",
    "# ######## Feature Cleaning\n",
    "# ## Sort feature: drop index\n",
    "# df_trajecory = feature_cleaning(df_trajecory)\n",
    "\n",
    "######## Merge Categorical and Numerical Feature\n",
    "## Merge \n",
    "df_trajecory = pd.concat([df_trajecory_categorical,\n",
    "                          df_trajecory_numerical],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Train-Test Split\n",
    "X = df_trajecory.values\n",
    "y = df_destination.label.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Save Data\n",
    "import h5py\n",
    "# Train data\n",
    "f = h5py.File(\"Train.hd5\", \"w\")\n",
    "f.create_dataset(\"data\", data=X_train,  compression=\"gzip\", compression_opts=4)\n",
    "f.create_dataset(\"label\", data=y_train,  compression=\"gzip\", compression_opts=4)\n",
    "f.close()\n",
    " \n",
    "#Test data\n",
    " \n",
    "f = h5py.File(\"Test.hd5\", \"w\")\n",
    "f.create_dataset(\"data\", data=X_test,  compression=\"gzip\", compression_opts=4)\n",
    "f.create_dataset(\"label\", data=y_test,  compression=\"gzip\", compression_opts=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ######## Load Data\n",
    "# import h5py\n",
    "# # Train data\n",
    "# # 读方式打开文件\n",
    "# file=h5py.File('./hdf5/Train.hd5','r')\n",
    "# # 尽管后面有 '[:]', 但是矩阵怎么进去的就是怎么出来的，不会被拉长（matlab后遗症）\n",
    "# X_train = file['data'][:]\n",
    "# y_train = file['label'][:]\n",
    "# file.close()\n",
    "    \n",
    "\n",
    "# # Test data\n",
    "# # 读方式打开文件\n",
    "# file=h5py.File('./hdf5/Test.hd5','r')\n",
    "# # 尽管后面有 '[:]', 但是矩阵怎么进去的就是怎么出来的，不会被拉长（matlab后遗症）\n",
    "# X_test = file['data'][:]\n",
    "# y_test = file['label'][:]\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Dropout, Activation  \n",
    "from keras.optimizers import SGD    \n",
    "from keras.utils import np_utils\n",
    "\n",
    "model = Sequential()  \n",
    "model.add(Dense(input_dim=X_train.shape[1], output_dim=500, init='glorot_uniform')) # 输入层， 2kpoints + embedding  \n",
    "model.add(Activation('tanh')) # 激活函数是tanh  \n",
    "model.add(Dropout(0.5)) # 采用50%的dropout\n",
    "\n",
    "model.add(Dense(input_dim=500, output_dim=500, init='glorot_uniform')) # 隐层节点500个  \n",
    "model.add(Activation('relu'))  \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(input_dim=500, output_dim=len(np.unique(y)), init='glorot_uniform')) # 输出结果类别数量就是维度 \n",
    "# model.add(Dense(input_dim=500, output_dim=200, init='glorot_uniform')) # 输出结果类别数量就是维度 \n",
    "model.add(Activation('softmax')) # 最后一层用softmax\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # 设定学习率（lr）等参数  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用Havasine距离作为loss函数\n",
    "\n",
    "#### for softmax layer\n",
    "nb_classes = len(np.unique(y))\n",
    "# y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "# y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "y_train = (np.arange(nb_classes) == y_train[:, None]).astype(int) # 参考上一篇文章，这里需要把index转换成一个one hot的矩阵  \n",
    "y_test = (np.arange(nb_classes) == y_test[:, None]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 219182 samples, validate on 93936 samples\n",
      "Epoch 1/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.9581 - val_loss: 0.8747\n",
      "Epoch 2/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.8943 - val_loss: 0.8499\n",
      "Epoch 3/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.8218 - val_loss: 0.6881\n",
      "Epoch 4/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.6657 - val_loss: 0.5595\n",
      "Epoch 5/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.5849 - val_loss: 0.5109\n",
      "Epoch 6/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.5463 - val_loss: 0.4932\n",
      "Epoch 7/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.5216 - val_loss: 0.4630\n",
      "Epoch 8/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.5018 - val_loss: 0.4522\n",
      "Epoch 9/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.4813 - val_loss: 0.4249\n",
      "Epoch 10/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.4609 - val_loss: 0.3930\n",
      "Epoch 11/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.4360 - val_loss: 0.3749\n",
      "Epoch 12/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.4122 - val_loss: 0.3490\n",
      "Epoch 13/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3940 - val_loss: 0.3315\n",
      "Epoch 14/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3782 - val_loss: 0.3201\n",
      "Epoch 15/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3629 - val_loss: 0.3059\n",
      "Epoch 16/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3522 - val_loss: 0.2929\n",
      "Epoch 17/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3413 - val_loss: 0.2832\n",
      "Epoch 18/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3300 - val_loss: 0.2746\n",
      "Epoch 19/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3230 - val_loss: 0.2814\n",
      "Epoch 20/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3154 - val_loss: 0.2689\n",
      "Epoch 21/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3088 - val_loss: 0.2535\n",
      "Epoch 22/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.3018 - val_loss: 0.2431\n",
      "Epoch 23/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2956 - val_loss: 0.2419\n",
      "Epoch 24/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2904 - val_loss: 0.2423\n",
      "Epoch 25/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2868 - val_loss: 0.2404\n",
      "Epoch 26/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2812 - val_loss: 0.2389\n",
      "Epoch 27/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2780 - val_loss: 0.2215\n",
      "Epoch 28/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2752 - val_loss: 0.2536\n",
      "Epoch 29/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2712 - val_loss: 0.2156\n",
      "Epoch 30/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2677 - val_loss: 0.2105\n",
      "Epoch 31/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2644 - val_loss: 0.2098\n",
      "Epoch 32/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2608 - val_loss: 0.2011\n",
      "Epoch 33/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2593 - val_loss: 0.1996\n",
      "Epoch 34/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2568 - val_loss: 0.2094\n",
      "Epoch 35/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2551 - val_loss: 0.2094\n",
      "Epoch 36/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2518 - val_loss: 0.1943\n",
      "Epoch 37/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2499 - val_loss: 0.2238\n",
      "Epoch 38/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2486 - val_loss: 0.2094\n",
      "Epoch 39/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2463 - val_loss: 0.1953\n",
      "Epoch 40/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2447 - val_loss: 0.1946\n",
      "Epoch 41/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2436 - val_loss: 0.1864\n",
      "Epoch 42/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2422 - val_loss: 0.1875\n",
      "Epoch 43/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2394 - val_loss: 0.2083\n",
      "Epoch 44/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2379 - val_loss: 0.2205\n",
      "Epoch 45/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2386 - val_loss: 0.2307\n",
      "Epoch 46/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2372 - val_loss: 0.2078\n",
      "Epoch 47/100\n",
      "219182/219182 [==============================] - 4s - loss: 0.2363 - val_loss: 0.1896\n",
      "Epoch 48/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2343 - val_loss: 0.1916\n",
      "Epoch 49/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2322 - val_loss: 0.1828\n",
      "Epoch 50/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2324 - val_loss: 0.1979\n",
      "Epoch 51/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2314 - val_loss: 0.1881\n",
      "Epoch 52/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2291 - val_loss: 0.1922\n",
      "Epoch 53/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2289 - val_loss: 0.1829\n",
      "Epoch 54/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2285 - val_loss: 0.2099\n",
      "Epoch 55/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2265 - val_loss: 0.1973\n",
      "Epoch 56/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2268 - val_loss: 0.1834\n",
      "Epoch 57/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2249 - val_loss: 0.1816\n",
      "Epoch 58/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2244 - val_loss: 0.1725\n",
      "Epoch 59/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2235 - val_loss: 0.1791\n",
      "Epoch 60/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2236 - val_loss: 0.1916\n",
      "Epoch 61/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2222 - val_loss: 0.1819\n",
      "Epoch 62/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2213 - val_loss: 0.1871\n",
      "Epoch 63/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2203 - val_loss: 0.1744\n",
      "Epoch 64/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2206 - val_loss: 0.1798\n",
      "Epoch 65/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2198 - val_loss: 0.1712\n",
      "Epoch 66/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2182 - val_loss: 0.1702\n",
      "Epoch 67/100\n",
      "219182/219182 [==============================] - 4s - loss: 0.2183 - val_loss: 0.1807\n",
      "Epoch 68/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2166 - val_loss: 0.1710\n",
      "Epoch 69/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2178 - val_loss: 0.1721\n",
      "Epoch 70/100\n",
      "219182/219182 [==============================] - 3s - loss: 0.2157 - val_loss: 0.1749\n",
      "Epoch 71/100\n",
      "219182/219182 [==============================] - 4s - loss: 0.2156 - val_loss: 0.1761\n",
      "Epoch 72/100\n",
      "219182/219182 [==============================] - 4s - loss: 0.2149 - val_loss: 0.1766\n",
      "Epoch 73/100\n",
      " 21200/219182 [=>............................] - ETA: 3s - loss: 0.2205"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-af887bd5b24f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# validation_split就是拿出百分之多少用来做交叉验证\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m model.fit(X_train, y_train, batch_size=200, nb_epoch=100, \n\u001b[1;32m----> 7\u001b[1;33m           shuffle=True, verbose=1, show_accuracy=True, validation_split=0.3)  \n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'test set'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jiandong/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/home/jiandong/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1105\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jiandong/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    812\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m                         \u001b[1;31m# do not slice the training phase flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jiandong/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mslice_X\u001b[1;34m(X, start, stop)\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练，这里参数比较多。batch_size就是batch_size，nb_epoch就是最多迭代的次数， shuffle就是是否把数据随机打乱之后再进行训练\n",
    "# verbose是屏显模式，官方这么说的：verbose: 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "# 就是说0是不屏显，1是显示一个进度条，2是每个epoch都显示一行数据\n",
    "# show_accuracy就是显示每次迭代后的正确率\n",
    "# validation_split就是拿出百分之多少用来做交叉验证\n",
    "model.fit(X_train, y_train, batch_size=200, nb_epoch=100, \n",
    "          shuffle=True, verbose=1, show_accuracy=True, validation_split=0.3)  \n",
    "print 'test set'  \n",
    "model.evaluate(X_test, y_test, batch_size=200, show_accuracy=True, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
